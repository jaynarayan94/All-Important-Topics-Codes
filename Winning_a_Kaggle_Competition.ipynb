{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Winning a Kaggle Competition in Python.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaynarayan94/All-Important-Topics-Codes/blob/master/Winning_a_Kaggle_Competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUBPVHt9VbIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Import log_loss from sklearn\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Define your own LogLoss function\n",
        "def own_logloss(y_true, prob_pred):\n",
        "  \t# Find loss for each observation\n",
        "    terms = y_true * np.log(prob_pred) + (1 - y_true) * np.log(1 - prob_pred)\n",
        "    # Find mean over all observations\n",
        "    err = np.mean(terms)   \n",
        "    return -err\n",
        "\n",
        "print('Sklearn LogLoss: {:.5f}'.format(log_loss(y_classification_true, y_classification_pred)))\n",
        "print('Your LogLoss: {:.5f}'.format(own_logloss(y_classification_true, y_classification_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0X1AYjK05Io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import KFold\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Create a KFold object\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
        "\n",
        "# Loop through each split\n",
        "fold = 0\n",
        "for train_index, test_index in kf.split(train):\n",
        "    # Obtain training and testing folds\n",
        "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "    print('Fold: {}'.format(fold))\n",
        "    print('CV train shape: {}'.format(cv_train.shape))\n",
        "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
        "    fold += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjUrURBJ05L8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Create a StratifiedKFold object\n",
        "str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
        "\n",
        "# Loop through each split\n",
        "fold = 0\n",
        "for train_index, test_index in str_kf.split(train, train['interest_level']):\n",
        "    # Obtain training and testing folds\n",
        "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "    print('Fold: {}'.format(fold))\n",
        "    print('CV train shape: {}'.format(cv_train.shape))\n",
        "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
        "    fold += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS-mza3y05Rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create TimeSeriesSplit object\n",
        "time_kfold = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Sort train data by date\n",
        "train = train.sort_values(by='date')\n",
        "\n",
        "# Iterate through each split\n",
        "fold = 0\n",
        "for train_index, test_index in time_kfold.split(train):\n",
        "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "    \n",
        "    print('Fold :', fold)\n",
        "    print('Train date range: from {} to {}'.format(cv_train.date.min(), cv_train.date.max()))\n",
        "    print('Test date range: from {} to {}\\n'.format(cv_test.date.min(), cv_test.date.max()))\n",
        "    fold += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnJ5TnkJ05UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np\n",
        "\n",
        "# Sort train data by date\n",
        "train = train.sort_values('date')\n",
        "\n",
        "# Initialize 3-fold time cross-validation\n",
        "kf = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Get MSE scores for each cross-validation split\n",
        "mse_scores = get_fold_mse(train, kf)\n",
        "\n",
        "print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))\n",
        "print('MSE by fold: {}'.format(mse_scores))\n",
        "print('Overall validation MSE: {:.5f}'.format(np.mean(mse_scores) + np.std(mse_scores)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-ubNyN205Xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the initial RMSE\n",
        "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
        "\n",
        "# Add total area of the house\n",
        "train['TotalArea'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\n",
        "\n",
        "# Look at the updated RMSE\n",
        "print('RMSE with total area:', get_kfold_rmse(train))\n",
        "\n",
        "# Look at the initial RMSE\n",
        "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
        "\n",
        "# Add total area of the house\n",
        "train['TotalArea'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\n",
        "print('RMSE with total area:', get_kfold_rmse(train))\n",
        "\n",
        "# Add garder area of the property\n",
        "train['GardenArea'] = train['LotArea'] - train['1stFlrSF']\n",
        "print('RMSE with garden area:', get_kfold_rmse(train))\n",
        "\n",
        "# Add total number of bathrooms\n",
        "train['TotalBath'] = train['FullBath'] + train['HalfBath']\n",
        "print('RMSE with number of bathrooms:', get_kfold_rmse(train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLupUXG705av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Concatenate train and test together\n",
        "taxi = pd.concat([train, test])\n",
        "\n",
        "# Convert pickup date to datetime object\n",
        "taxi['pickup_datetime'] = pd.to_datetime(taxi['pickup_datetime'])\n",
        "\n",
        "# Create day of week feature\n",
        "taxi['dayofweek'] = taxi['pickup_datetime'].dt.dayofweek\n",
        "\n",
        "# Create hour feature\n",
        "taxi['hour'] = taxi['pickup_datetime'].dt.hour\n",
        "\n",
        "# Split back into train and test\n",
        "new_train = taxi[taxi['id'].isin(train['id'])]\n",
        "new_test = taxi[taxi['id'].isin(test['id'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yED7NKmxUu8x",
        "colab_type": "text"
      },
      "source": [
        "# Mean target encoding\n",
        "First of all, you will create a function that implements mean target encoding. Remember that you need to develop the two following steps:\n",
        "\n",
        "1. Calculate the mean on the train, apply to the test\n",
        "2. Split train into K folds. Calculate the out-of-fold mean for each fold, apply to this particular fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwSwO5KY05dS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_mean_target_encoding(train, test, target, categorical, alpha=5):\n",
        "    # Calculate global mean on the train data\n",
        "    global_mean = train[target].mean()\n",
        "    \n",
        "    # Group by the categorical feature and calculate its properties\n",
        "    train_groups = train.groupby(categorical)\n",
        "    category_sum = train_groups[target].sum()\n",
        "    category_size = train_groups.size()\n",
        "    \n",
        "    # Calculate smoothed mean target statistics\n",
        "    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)\n",
        "    \n",
        "    # Apply statistics to the test data and fill new categories\n",
        "    test_feature = test[categorical].map(train_statistics).fillna(global_mean)\n",
        "    return test_feature.values\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSDnxtHg05jG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_mean_target_encoding(train, target, categorical, alpha=5):\n",
        "    # Create 5-fold cross-validation\n",
        "    kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
        "    train_feature = pd.Series(index=train.index)\n",
        "    \n",
        "    # For each folds split\n",
        "    for train_index, test_index in kf.split(train):\n",
        "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "      \n",
        "        # Calculate out-of-fold statistics and apply to cv_test\n",
        "        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)\n",
        "        \n",
        "        # Save new feature for this particular fold\n",
        "        train_feature.iloc[test_index] = cv_test_feature       \n",
        "    return train_feature.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYise1HW05ph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_target_encoding(train, test, target, categorical, alpha=5):\n",
        "  \n",
        "    # Get test feature\n",
        "    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)\n",
        "    \n",
        "    # Get train feature\n",
        "    train_feature = train_mean_target_encoding(train, target, categorical, alpha)\n",
        "    \n",
        "    # Return new features to add to the model\n",
        "    return train_feature, test_feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLfIg9TA05u-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
        "\n",
        "# For each folds split\n",
        "for train_index, test_index in kf.split(bryant_shots):\n",
        "    cv_train, cv_test = bryant_shots.iloc[train_index], bryant_shots.iloc[test_index]\n",
        "\n",
        "    # Create mean target encoded feature\n",
        "    cv_train['game_id_enc'], cv_test['game_id_enc'] = mean_target_encoding(train=cv_train,\n",
        "                                                                           test=cv_test,\n",
        "                                                                           target='shot_made_flag',\n",
        "                                                                           categorical='game_id',\n",
        "                                                                           alpha=5)\n",
        "    # Look at the encoding\n",
        "    print(cv_train[['game_id', 'shot_made_flag', 'game_id_enc']].sample(n=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKJdksPU051C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import SimpleImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create mean imputer\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Price imputation\n",
        "rental_listings[['price']] = mean_imputer.fit_transform(rental_listings[['price']])\n",
        "\n",
        "# Import SimpleImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create constant imputer\n",
        "constant_imputer = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
        "\n",
        "# building_id imputation\n",
        "rental_listings[['building_id']] = constant_imputer.fit_transform(rental_listings[['building_id']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBm4n5kNPJBC",
        "colab_type": "text"
      },
      "source": [
        "## Baseline prediction based on the date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwk3vGEs054Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Calculate average fare_amount on the validation_train data\n",
        "naive_prediction = np.mean(validation_train.fare_amount)\n",
        "\n",
        "# Assign naive prediction to all the holdout observations\n",
        "validation_test['pred'] = naive_prediction\n",
        "\n",
        "# Measure the local RMSE\n",
        "rmse = sqrt(mean_squared_error(validation_test['fare_amount'], validation_test['pred']))\n",
        "print('Validation RMSE for Baseline I model: {:.3f}'.format(rmse))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8esJEbs05y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get pickup hour from the pickup_datetime column\n",
        "train['hour'] = train['pickup_datetime'].dt.hour\n",
        "test['hour'] = test['pickup_datetime'].dt.hour\n",
        "\n",
        "# Calculate average fare_amount by pickup hour \n",
        "hour_groups = train.groupby('hour').fare_amount.mean()\n",
        "\n",
        "# Make predicitons on the test set\n",
        "test['fare_amount'] = test.hour.map(hour_groups)\n",
        "\n",
        "# Write predictions\n",
        "test[['id','fare_amount']].to_csv('hour_mean_sub.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grDfT7ZUP1u3",
        "colab_type": "text"
      },
      "source": [
        "## Baseline based on the gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR3Z2Gz605nI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Select only numeric features\n",
        "features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
        "            'dropoff_latitude', 'passenger_count', 'hour']\n",
        "\n",
        "# Train a Random Forest model\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(train[features], train.fare_amount)\n",
        "\n",
        "# Make predictions on the test data\n",
        "test['fare_amount'] = rf.predict(test[features])\n",
        "\n",
        "# Write predictions\n",
        "test[['id','fare_amount']].to_csv('rf_sub.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXmqJRa9Ub9J",
        "colab_type": "text"
      },
      "source": [
        "# Model blending\n",
        "That was not too hard! Blending allows you to get additional score improvements almost for free just by averaging multiple models predictions. Now, let's explore model stacking!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6eD6IUT05hO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "\n",
        "# Train a Gradient Boosting model\n",
        "gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)\n",
        "\n",
        "# Train a Random Forest model\n",
        "rf = RandomForestRegressor().fit(train[features], train.fare_amount)\n",
        "\n",
        "# Make predictions on the test data\n",
        "test['gb_pred'] = gb.predict(test[features])\n",
        "test['rf_pred'] = rf.predict(test[features])\n",
        "\n",
        "# Find mean of model predictions\n",
        "test['blend'] = (test['gb_pred'] + test['rf_pred'] ) / 2\n",
        "print(test[['gb_pred', 'rf_pred', 'blend']].head(3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXCRa_WcVUFx",
        "colab_type": "text"
      },
      "source": [
        "# Model stacking I\n",
        "Now it's time for stacking. To implement the stacking approach, you will follow the 6 steps we've discussed in the previous video:\n",
        "\n",
        "1. Split train data into two parts\n",
        "2.Train multiple models on Part 1\n",
        "3.Make predictions on Part 2\n",
        "4.Make predictions on the test data\n",
        "5. Train a new model on Part 2 using predictions as features\n",
        "6.Make predictions on the test data using the 2nd level model\n",
        "\n",
        "Congratulations, now your toolbox contains ensembling techniques! Usually, the 2nd level model is some simple model like Linear or Logistic Regressions. Also, note that you were not using intercept in the Linear Regression just to combine pure model predictions. Looking at the coefficients, it's clear that 2nd level model has more trust to the Gradient Boosting: 0.7 versus 0.3 for the Random Forest model. Now, move forward to the last lesson in order to learn some final tips and tricks!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrfjN1KgUev6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "\n",
        "# Split train data into two parts\n",
        "part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)\n",
        "\n",
        "# Train a Gradient Boosting model on Part 1\n",
        "gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\n",
        "\n",
        "# Train a Random Forest model on Part 1\n",
        "rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)\n",
        "\n",
        "# 2----------------------------------\n",
        "\n",
        "# Make predictions on the Part 2 data\n",
        "part_2['gb_pred'] = gb.predict(part_2[features])\n",
        "part_2['rf_pred'] = rf.predict(part_2[features])\n",
        "\n",
        "# Make predictions on the test data\n",
        "test['gb_pred'] = gb.predict(test[features])\n",
        "test['rf_pred'] = rf.predict(test[features])\n",
        "\n",
        "#-------------------------------------------------\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create linear regression model without the intercept\n",
        "lr = LinearRegression(fit_intercept=False)\n",
        "\n",
        "# Train 2nd level model in the part_2 data\n",
        "lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)\n",
        "\n",
        "# Make stacking predictions on the test data\n",
        "test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])\n",
        "\n",
        "# Look at the model coefficients\n",
        "print(lr.coef_)\n",
        "\n",
        "# lr.coef_ =  [0.72504358 0.27647395]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZciySwHUezR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}